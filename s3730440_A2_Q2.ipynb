{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "# Question 2\n",
        "\n",
        "We are using \"A2_Q2.csv\" dataset, which resides in the same folder.\n",
        "\n",
        "## Part A\n",
        "We assume *true* is positive target level. Using a score threshold of 0.5 we calculate confusion matrix using `pd.crosstab()`<br>\n",
        "We import our dataset into pandas dataframe."
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('A2_Q2.csv')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "(30, 3)"
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "We now analyse **Target** class for correct and incorrect classifications. We assume correct classification is any **Score >= 0.5**"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "df[df.Score >= 0.50]['Target'].value_counts()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "True     9\nFalse    6\nName: Target, dtype: int64"
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "df[df.Score < 0.50]['Target'].value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "False    11\nTrue      4\nName: Target, dtype: int64"
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Create copy of our dataframe, rename column **Target** to **Predicted** and display this df"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "df_new = df.copy()\n",
        "df_new = df_new.rename(columns={'Target':'Predicted'})\n",
        "df_new.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "   ID  Predicted  Score\n0   1      False   0.46\n1   2      False   0.14\n2   3      False   0.48\n3   4       True   0.91\n4   5      False   0.24",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Predicted</th>\n      <th>Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>False</td>\n      <td>0.46</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>False</td>\n      <td>0.14</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>False</td>\n      <td>0.48</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>True</td>\n      <td>0.91</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>False</td>\n      <td>0.24</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "We create a new column **Actual** which is **True** for all score thresholds >= 0.5, **False** otherwise"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "df_new['Actual'] = df_new['Score'] >= 0.50\n",
        "df_new.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "   ID  Predicted  Score  Actual\n0   1      False   0.46   False\n1   2      False   0.14   False\n2   3      False   0.48   False\n3   4       True   0.91    True\n4   5      False   0.24   False",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Predicted</th>\n      <th>Score</th>\n      <th>Actual</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>False</td>\n      <td>0.46</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>False</td>\n      <td>0.14</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>False</td>\n      <td>0.48</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>True</td>\n      <td>0.91</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>False</td>\n      <td>0.24</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Create confusion matrix for **Actual** vs **Predicted** results"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "confusion_matrix = pd.crosstab(df_new['Predicted'], df_new['Actual'], rownames=['Predicted'], colnames=['Actual'])\n",
        "confusion_matrix"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "Actual     False  True \nPredicted              \nFalse         11      6\nTrue           4      9",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Actual</th>\n      <th>False</th>\n      <th>True</th>\n    </tr>\n    <tr>\n      <th>Predicted</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>False</th>\n      <td>11</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>True</th>\n      <td>4</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "## Part B\n",
        "In this part we have to calculate following 5 metrics in *df_metric* dataframe:<br>\n",
        "- Error Rate\n",
        "- Precision\n",
        "- Recall\n",
        "- F1 Score\n",
        "- TNR\n",
        "- FPR"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "**Error Rate**"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "error_rate = (confusion_matrix[0][1]+confusion_matrix[1][0])/(confusion_matrix.values.sum())\n",
        "error_rate"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.3333333333333333"
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "**Precision**"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "precision = (confusion_matrix[1][1])/ (confusion_matrix[1].values.sum())\n",
        "precision"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.6"
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "**Recall a.k.a TPR**"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "recall = (confusion_matrix[1][1]) / (confusion_matrix.iloc[1].values.sum())\n",
        "recall"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.6923076923076923"
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "**F1 Score**"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "f1_score = (2 * precision * recall) / (precision + recall)\n",
        "f1_score"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.6428571428571429"
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "**TNR**"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "tnr = confusion_matrix[0][0] / (confusion_matrix.iloc[0].values.sum())\n",
        "tnr"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.6470588235294118"
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "**FPR**"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "fpr = 1 - tnr\n",
        "fpr"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.3529411764705882"
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "df_metrics = pd.DataFrame(columns=['Metrics', 'Value'])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "df_metrics = df_metrics.append({'Metrics':'Error Rate', 'Value':error_rate.round(3)}, ignore_index=True)\n",
        "df_metrics = df_metrics.append({'Metrics':'Precision', 'Value':precision.round(3)}, ignore_index=True)\n",
        "df_metrics = df_metrics.append({'Metrics':'Recall', 'Value':recall.round(3)}, ignore_index=True)\n",
        "df_metrics = df_metrics.append({'Metrics':'F1 Score', 'Value':f1_score.round(3)}, ignore_index=True)\n",
        "df_metrics = df_metrics.append({'Metrics':'FPR', 'Value':fpr.round(3)}, ignore_index=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "df_metrics"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "      Metrics  Value\n0  Error Rate  0.333\n1   Precision  0.600\n2      Recall  0.692\n3    F1 Score  0.643\n4         FPR  0.353",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Metrics</th>\n      <th>Value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Error Rate</td>\n      <td>0.333</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Precision</td>\n      <td>0.600</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Recall</td>\n      <td>0.692</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>F1 Score</td>\n      <td>0.643</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>FPR</td>\n      <td>0.353</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "# Part C\n",
        "In this part we are varying the score threshold from 0.1 to 0.9 (both inclusive) with steps of 0.1 and compute **TPR** and **FPR** values"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "df_roc = pd.DataFrame(columns=['Threshold', 'TPR', 'FPR'])\n",
        "df_roc"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "Empty DataFrame\nColumns: [Threshold, TPR, FPR]\nIndex: []",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Threshold</th>\n      <th>TPR</th>\n      <th>FPR</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "We then append the calculated **TPR** and **FPR** values to `df_roc` dataframe."
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "df_two = df.copy()\n",
        "df_two = df_two.rename(columns={'Target':'Predicted'})\n",
        "\n",
        "# Calculate FPR & TPR at all thresholds from 0.1 to 0.9 (inclusive), with step increment of 0.1\n",
        "for threshold in np.arange(0.1, 1, 0.1).round(2):\n",
        "    df_temp = df_two.copy()\n",
        "    df_temp['Actual'] = df_temp[['Score']] >= threshold\n",
        "    print(\"Threshold: \", threshold)\n",
        "    print(\"------Confusion Matrix-------\")\n",
        "    confusion_matrix = pd.crosstab(df_temp['Predicted'], df_temp['Actual'], rownames=['Predicted'], colnames=['Actual'])\n",
        "    print(confusion_matrix, \"\\n=============================\")\n",
        "    recall = ((confusion_matrix[1][1]) / (confusion_matrix.iloc[1].values.sum())).round(3)\n",
        "    tnr = confusion_matrix[0][0] / (confusion_matrix.iloc[0].values.sum())\n",
        "    fpr = (1 - tnr).round(3)\n",
        "    df_roc = df_roc.append({'Threshold':threshold, 'TPR':recall, 'FPR':fpr}, ignore_index=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Threshold:  0.1\n------Confusion Matrix-------\nActual     False  True \nPredicted              \nFalse          1     16\nTrue           0     13 \n=============================\nThreshold:  0.2\n------Confusion Matrix-------\nActual     False  True \nPredicted              \nFalse          5     12\nTrue           0     13 \n=============================\nThreshold:  0.3\n------Confusion Matrix-------\nActual     False  True \nPredicted              \nFalse          7     10\nTrue           1     12 \n=============================\nThreshold:  0.4\n------Confusion Matrix-------\nActual     False  True \nPredicted              \nFalse          9      8\nTrue           2     11 \n=============================\nThreshold:  0.5\n------Confusion Matrix-------\nActual     False  True \nPredicted              \nFalse         11      6\nTrue           4      9 \n=============================\nThreshold:  0.6\n------Confusion Matrix-------\nActual     False  True \nPredicted              \nFalse         13      4\nTrue           5      8 \n=============================\nThreshold:  0.7\n------Confusion Matrix-------\nActual     False  True \nPredicted              \nFalse         15      2\nTrue           7      6 \n=============================\nThreshold:  0.8\n------Confusion Matrix-------\nActual     False  True \nPredicted              \nFalse         16      1\nTrue           9      4 \n=============================\nThreshold:  0.9\n------Confusion Matrix-------\nActual     False  True \nPredicted              \nFalse         17      0\nTrue          12      1 \n=============================\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Here is what `df_roc` dataframe looks like"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "df_roc"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "   Threshold    TPR    FPR\n0        0.1  1.000  0.941\n1        0.2  1.000  0.706\n2        0.3  0.923  0.588\n3        0.4  0.846  0.471\n4        0.5  0.692  0.353\n5        0.6  0.615  0.235\n6        0.7  0.462  0.118\n7        0.8  0.308  0.059\n8        0.9  0.077  0.000",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Threshold</th>\n      <th>TPR</th>\n      <th>FPR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.1</td>\n      <td>1.000</td>\n      <td>0.941</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.2</td>\n      <td>1.000</td>\n      <td>0.706</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.3</td>\n      <td>0.923</td>\n      <td>0.588</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.4</td>\n      <td>0.846</td>\n      <td>0.471</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.5</td>\n      <td>0.692</td>\n      <td>0.353</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.6</td>\n      <td>0.615</td>\n      <td>0.235</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.7</td>\n      <td>0.462</td>\n      <td>0.118</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.8</td>\n      <td>0.308</td>\n      <td>0.059</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.9</td>\n      <td>0.077</td>\n      <td>0.000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": [
        "## Part D"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Using the above `df_roc`, we now display an **ROC Curve** with appropriate axes labels and a title."
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "#!pip install vega vega_datasets\n",
        "#!pip install --upgrade altair"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": [
        "# Code referenced from https://www.featureranking.com/tutorials/machine-learning-tutorials/sk-part-4-model-evaluation/\n",
        "\n",
        "\n",
        "import altair as alt\n",
        "# use alt.renderers.enable('html') if running locally, alt.renderers.enable('notebook') if running on Azure notebooks\n",
        "alt.renderers.enable('html')\n",
        "\n",
        "base = alt.Chart(df_roc, \n",
        "                 title='ROC Curve of df_roc'\n",
        "                ).properties(width=300)\n",
        "\n",
        "roc_curve = base.mark_line(point=True).encode(\n",
        "    alt.X('FPR', title='False Positive Rate (FPR)',  sort=None),\n",
        "    alt.Y('TPR', title='True Positive Rate (TPR) (a.k.a Recall)'),\n",
        ")\n",
        "\n",
        "roc_rule = base.mark_line(color='green').encode(\n",
        "    x='FPR',\n",
        "    y='FPR',\n",
        "    size=alt.value(2)\n",
        ")\n",
        "\n",
        "(roc_curve + roc_rule).interactive()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": "\n<div id=\"altair-viz-425b07d080c84db486e1098ef98a5ff5\"></div>\n<script type=\"text/javascript\">\n  (function(spec, embedOpt){\n    let outputDiv = document.currentScript.previousElementSibling;\n    if (outputDiv.id !== \"altair-viz-425b07d080c84db486e1098ef98a5ff5\") {\n      outputDiv = document.getElementById(\"altair-viz-425b07d080c84db486e1098ef98a5ff5\");\n    }\n    const paths = {\n      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n    };\n\n    function loadScript(lib) {\n      return new Promise(function(resolve, reject) {\n        var s = document.createElement('script');\n        s.src = paths[lib];\n        s.async = true;\n        s.onload = () => resolve(paths[lib]);\n        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n        document.getElementsByTagName(\"head\")[0].appendChild(s);\n      });\n    }\n\n    function showError(err) {\n      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n      throw err;\n    }\n\n    function displayChart(vegaEmbed) {\n      vegaEmbed(outputDiv, spec, embedOpt)\n        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n    }\n\n    if(typeof define === \"function\" && define.amd) {\n      requirejs.config({paths});\n      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n    } else if (typeof vegaEmbed === \"function\") {\n      displayChart(vegaEmbed);\n    } else {\n      loadScript(\"vega\")\n        .then(() => loadScript(\"vega-lite\"))\n        .then(() => loadScript(\"vega-embed\"))\n        .catch(showError)\n        .then(() => displayChart(vegaEmbed));\n    }\n  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": {\"type\": \"line\", \"point\": true}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"FPR\", \"sort\": null, \"title\": \"False Positive Rate (FPR)\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"TPR\", \"title\": \"True Positive Rate (TPR) (a.k.a Recall)\"}}, \"selection\": {\"selector001\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"title\": \"ROC Curve of df_roc\", \"width\": 300}, {\"mark\": {\"type\": \"line\", \"color\": \"green\"}, \"encoding\": {\"size\": {\"value\": 2}, \"x\": {\"type\": \"quantitative\", \"field\": \"FPR\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"FPR\"}}, \"title\": \"ROC Curve of df_roc\", \"width\": 300}], \"data\": {\"name\": \"data-a3c135c6969e085cbbcbb4fb676f460e\"}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-a3c135c6969e085cbbcbb4fb676f460e\": [{\"Threshold\": 0.1, \"TPR\": 1.0, \"FPR\": 0.941}, {\"Threshold\": 0.2, \"TPR\": 1.0, \"FPR\": 0.706}, {\"Threshold\": 0.3, \"TPR\": 0.923, \"FPR\": 0.588}, {\"Threshold\": 0.4, \"TPR\": 0.846, \"FPR\": 0.471}, {\"Threshold\": 0.5, \"TPR\": 0.692, \"FPR\": 0.353}, {\"Threshold\": 0.6, \"TPR\": 0.615, \"FPR\": 0.235}, {\"Threshold\": 0.7, \"TPR\": 0.462, \"FPR\": 0.118}, {\"Threshold\": 0.8, \"TPR\": 0.308, \"FPR\": 0.059}, {\"Threshold\": 0.9, \"TPR\": 0.077, \"FPR\": 0.0}]}}, {\"mode\": \"vega-lite\"});\n</script>",
            "text/plain": "alt.LayerChart(...)"
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Above is the **ROC Curve** for our dataset at all thresholds from 0.1 to 0.9 (both inclusive) at step interval of 0.1"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "## Question 2 wrap-up"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "df_metrics"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "      Metrics  Value\n0  Error Rate  0.333\n1   Precision  0.600\n2      Recall  0.692\n3    F1 Score  0.643\n4         FPR  0.353",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Metrics</th>\n      <th>Value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Error Rate</td>\n      <td>0.333</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Precision</td>\n      <td>0.600</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Recall</td>\n      <td>0.692</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>F1 Score</td>\n      <td>0.643</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>FPR</td>\n      <td>0.353</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "df_roc"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "   Threshold    TPR    FPR\n0        0.1  1.000  0.941\n1        0.2  1.000  0.706\n2        0.3  0.923  0.588\n3        0.4  0.846  0.471\n4        0.5  0.692  0.353\n5        0.6  0.615  0.235\n6        0.7  0.462  0.118\n7        0.8  0.308  0.059\n8        0.9  0.077  0.000",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Threshold</th>\n      <th>TPR</th>\n      <th>FPR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.1</td>\n      <td>1.000</td>\n      <td>0.941</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.2</td>\n      <td>1.000</td>\n      <td>0.706</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.3</td>\n      <td>0.923</td>\n      <td>0.588</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.4</td>\n      <td>0.846</td>\n      <td>0.471</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.5</td>\n      <td>0.692</td>\n      <td>0.353</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.6</td>\n      <td>0.615</td>\n      <td>0.235</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.7</td>\n      <td>0.462</td>\n      <td>0.118</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.8</td>\n      <td>0.308</td>\n      <td>0.059</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.9</td>\n      <td>0.077</td>\n      <td>0.000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python37764bitvenvvenv6d9ffd4397b446f5a5c6b0435d620f74",
      "display_name": "Python 3.7.7 64-bit ('venv': venv)",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7-final",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}